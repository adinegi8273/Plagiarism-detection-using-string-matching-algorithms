# -*- coding: utf-8 -*-
"""BagOfWords.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MgpBJ37_nveYLTh8sX4I0Qb19lsZ4ms0
"""

import string
import PyPDF2
import nltk
import re
from nltk.corpus import stopwords
from nltk.stem.porter import  PorterStemmer#stemming
from nltk.stem import WordNetLemmatizer #lemmatization

import PyPDF2

# Open a PDF file
with open('elephant.pdf', 'rb') as pdf_file:
    reader = PyPDF2.PdfReader(pdf_file)

    # Get the total number of pages
    total_pages = len(reader.pages)
    print(f"Total pages: {total_pages}")

    # Read the content of the first page
    first_page = reader.pages[0]
    text = first_page.extract_text()

print(text)

sentences = nltk.sent_tokenize(text) #tokenization
print(sentences)
ps = PorterStemmer()
corpus = []
for i in range(len(sentences)):
  review = re.sub('[^a-zA-Z]',' ',sentences[i])
  review = review.lower()
  review = review.split()
  review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]
  review = ' '.join(review)
  corpus.append(review)

print(corpus)


#our project needs computation till here (removing stopwords/converting to lowercase/stemming or lemmatization)

# ... [your existing code above remains unchanged]

# Save preprocessed corpus to a file
with open("pre-processed.txt", "w", encoding="utf-8") as file:
    for line in corpus:
        file.write(line + "\n")  # Write each sentence on a new line

print("Preprocessed data has been saved to 'pre-processed.txt'")

